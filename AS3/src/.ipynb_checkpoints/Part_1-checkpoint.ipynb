{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Import models\n",
    "from implementation import LogisticRegression2Class, LogisticRegressionMultiClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Class Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing Data\n",
    "\"\"\"\n",
    "# Retrieve data\n",
    "data = pd.read_csv('data_banknote_authentication.txt',header=None).values\n",
    "\n",
    "# Shuffle and split into train/test split\n",
    "data = shuffle(data,random_state=0)\n",
    "l = data.shape[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:(l-1)],data[:,-1],test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 5-fold cross validation across 3 different models:\n",
      "Logistic Regression with original features:\n",
      "\tAverage training f1 score: 0.8590160986821302\n",
      "\tAverage testing f1 score: 0.8277957926131062\n",
      "Logistic Regression with Degree 2 polynomial mapping of features:\n",
      "\tAverage training f1 score: 0.8944071472551494\n",
      "\tAverage testing f1 score: 0.9115824172132593\n",
      "Logistic Regression with Degree 3 polynomial mapping of features:\n",
      "\tAverage training f1 score: 0.9577456723177298\n",
      "\tAverage testing f1 score: 0.9613551343884282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: divide by zero encountered in log\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:95: RuntimeWarning: invalid value encountered in multiply\n",
      "  J_theta = -(np.sum((y*np.log(h)) + ((1-y)*np.log(1-h))))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform KFold cross validation to validate and choose model\n",
    "\"\"\"\n",
    "kf = KFold(n_splits=5)\n",
    "degrees = [1,2,3]\n",
    "print(\"Performing 5-fold cross validation across 3 different models:\")\n",
    "\n",
    "average_test_f1 = []\n",
    "average_train_f1 = []\n",
    "\n",
    "for degree in degrees:\n",
    "    train_f1 = []\n",
    "    test_f1 = []\n",
    "    # Start KFold for current degree of polynomial\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        # Split to train and test for this fold\n",
    "        X_train_kfold, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_train_kfold, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # Create polynomial features\n",
    "        if degree != 1:\n",
    "            poly_reg = PolynomialFeatures(degree=degree)\n",
    "            X_train_poly = poly_reg.fit_transform(X_train_kfold)\n",
    "            X_test_poly = poly_reg.fit_transform(X_val)\n",
    "        else:\n",
    "            X_train_poly = X_train_kfold\n",
    "            X_test_poly = X_val\n",
    "\n",
    "        # Train 2-class Logistic Regression\n",
    "        lr = LogisticRegression2Class()\n",
    "        lr.fit(X_train_poly,y_train_kfold,epochs=30,learning_rate=1e-4)\n",
    "\n",
    "        # Predict\n",
    "        train_pred = lr.predict(X_train_poly)\n",
    "        test_pred = lr.predict(X_test_poly)\n",
    "\n",
    "        # Save the f1 of this fold\n",
    "        train_f1.append(f1_score(train_pred,y_train_kfold))\n",
    "        test_f1.append(f1_score(test_pred,y_val))\n",
    "\n",
    "    # Average the f1 across 5 fold for each mapping\n",
    "    average_test_f1.append(np.average(test_f1))\n",
    "    average_train_f1.append(np.average(train_f1))\n",
    "    \n",
    "for i, (avg_train_f1, avg_test_f1) in enumerate(zip(average_train_f1,average_test_f1)):\n",
    "    if degrees[i] == 1:\n",
    "        print(f\"Logistic Regression with original features:\")\n",
    "    else:\n",
    "        print(f\"Logistic Regression with Degree {degrees[i]} polynomial mapping of features:\")\n",
    "    print(f\"\\tAverage training f1 score: {avg_train_f1}\")\n",
    "    print(f\"\\tAverage testing f1 score: {avg_test_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-class LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Processing data\n",
    "\"\"\"\n",
    "# Image data vectorized\n",
    "train_data = pd.read_csv('fashion-mnist_train.csv')\n",
    "test_data = pd.read_csv('fashion-mnist_test.csv')\n",
    "\n",
    "# Convert to matrices\n",
    "X_train = train_data.iloc[:,1:].values\n",
    "y_train = train_data.label.values\n",
    "X_test = test_data.iloc[:,1:].values\n",
    "y_test = test_data.label.values\n",
    "\n",
    "# Normalize pixels\n",
    "X_train_normalized = X_train/255\n",
    "X_test_normalized = X_test/255\n",
    "\n",
    "# One hot encode labels\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(y_train.reshape(-1,1))\n",
    "y_train_ohe, y_test_ohe = ohe.transform(y_train.reshape(-1,1)), ohe.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:138: RuntimeWarning: divide by zero encountered in log\n",
      "  # Sum from numpy will sum across all axis\n",
      "/Users/tuantran/Desktop/cs584-m20-tuan-tran/AS3/src/implementation.py:138: RuntimeWarning: invalid value encountered in multiply\n",
      "  # Sum from numpy will sum across all axis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with original features:\n",
      "\tAverage training accuracy score: 74.89291666666666\n",
      "\tAverage testing accuracy score: 74.87333333333333\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KFold\n",
    "\"\"\"\n",
    "# Perform KFold cross validation to validate model\n",
    "# Can use accuracy since the dataset is balanced\n",
    "num_class = len(np.unique(y_train))\n",
    "kf = KFold(n_splits=5)\n",
    "degrees = [1]\n",
    "\n",
    "average_test_accuracies = []\n",
    "average_train_accuracies = []\n",
    "\n",
    "for degree in degrees:\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    # Start KFold for current degree of polynomial\n",
    "    for train_index, val_index in kf.split(X_train_normalized):\n",
    "        # Split to train and test for this fold\n",
    "        X_train_kfold, X_val = X_train_normalized[train_index], X_train_normalized[val_index]\n",
    "        y_train_kfold_ohe, y_val = y_train_ohe[train_index].toarray(), y_train[val_index]\n",
    "        y_train_kfold = y_train[train_index]\n",
    "        # Create polynomial features\n",
    "        if degree != 1:\n",
    "            poly_reg = PolynomialFeatures(degree=degree)\n",
    "            X_train_poly = poly_reg.fit_transform(X_train_kfold)\n",
    "            X_test_poly = poly_reg.fit_transform(X_val)\n",
    "        else:\n",
    "            X_train_poly = X_train_kfold\n",
    "            X_test_poly = X_val\n",
    "\n",
    "        # Train multi-class Logistic Regression\n",
    "        lr = LogisticRegressionMultiClass(num_class)\n",
    "        lr.fit(X_train_poly,y_train_kfold_ohe,epochs=200,learning_rate=1e-4)\n",
    "\n",
    "        # Predict\n",
    "        train_pred = lr.predict(X_train_poly)\n",
    "        test_pred = lr.predict(X_test_poly)\n",
    "\n",
    "        # Save the accuracy of this fold\n",
    "        train_accuracy.append(accuracy_score(train_pred,y_train_kfold))\n",
    "        test_accuracy.append(accuracy_score(test_pred,y_val))\n",
    "\n",
    "    # Average the accuracy across 5 fold for each mapping\n",
    "    average_test_accuracies.append(np.average(test_accuracy))\n",
    "    average_train_accuracies.append(np.average(train_accuracy))\n",
    "    \n",
    "for i, (average_train_accuracy, average_test_accuracy) in enumerate(zip(average_train_accuracies,average_test_accuracies)):\n",
    "    if degrees[i] == 1:\n",
    "        print(f\"Logistic Regression with original features:\")\n",
    "    else:\n",
    "        print(f\"Logistic Regression with Degree {degrees[i]} polynomial mapping of features:\")\n",
    "    print(f\"\\tAverage training accuracy score: {average_train_accuracy*100}\")\n",
    "    print(f\"\\tAverage testing accuracy score: {average_test_accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on final test set: 76.23\n"
     ]
    }
   ],
   "source": [
    "num_class = len(np.unique(y_train))\n",
    "lr = LogisticRegressionMultiClass(num_class)\n",
    "lr.fit(X_train_normalized,y_train_ohe.toarray(),epochs=200,learning_rate=1e-4)\n",
    "print(f\"Accuracy score on final test set: {accuracy_score(y_test,lr.predict(X_test_normalized))*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
